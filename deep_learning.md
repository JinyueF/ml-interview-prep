## Deep Learning Review

### Basics
#### Concepts
- MLP with non-linear activation functions can approximate any function given sufficient hidden units
- Deeper network reduces the requirement of units and improves generalization
- Cross entropy outcompetes mean squared error: 
    +  MSE results in small gradients and slow weight updates and more prone to vanishing gradient
    +  CE quickly update weights when the loss is large and slowly update weights when the loss is small
    
- Activation functions
    + ReLU = max(0,z)
        * almost linear
        * Large derivate in activated states
        * prevents saturated units which occurs at the ends of sigmoid function
        
    + Sigmoid and tanh
        * note the different between the output ranges (0,1) vs (-1, 1)
        * tanh typically performs better than Sigmoid
            - distribution mean = 0 --> stronger gradients
            - avoid bias in the gradients

    + softmax
        * generalized version of sigmoid
        * represents the probablity distribution of the possible output values


#### Solutions to Overfitting
- Augment data
- Reduce model complexity
    + NN: reduce the number of hidden layers/neurons
    + Decision tree: `Pruning` (restrict the size of the tree by removing sections that provide little power)
- Regularization
    + Batch Normalization
        * Normalize the distribution of the data - mean=0, SD=1
    + L1 and L2 regularizatin
        * In general: Retrict large weights, increase bias, reduce variancem, reduce the complexity of the model

        | Comparison | L1 | L2 |
        |---|---|---|
        | Mechanism of Overfitting Prevention | Sparsify Weight Matrices | Punish large weights |
        | Use Cases| When features are correlated | When features are not correlated| 

- Emsemble Learning
    - Dropout (for NN)
        + randomly dropping units to prevent units from co-adaption
        + Comparison to Bagging
            * Bagging: training different models on different sub-datasets and average their output
            * Dropout: neural nets after dropout inherent parameters from the parent network, so when training a subset, the other subsets also benefit from a good parameter setting
            
    - Random Forests (for decision trees)
        + construct a multitude of decision trees and output the mode of the classes (classification) / mean prediction (regression) generated by these decision trees

#### Optimization
##### Gradient Descent
- Stochastic Gradient Descent
    + Motivations to use mini-batch
        * reduce gradient variance and stablize the iterative process
        * utilize matrix computation to increase efficiency
        
    + Effects of batch size
        * larger size: 
            -   more accurate estimation of the gradients (faster convergence)
            -   but once the size reaches a certain limit, further increasing the sizes introduce no additional benefits
            
        * smaller size: 
            -   results in better generalization possibly because smaller sizes introduce more noises, which acts as regularization
            -   requires small learning rate due to the large variance
            -   requires long training time

    +   problems with SGD
        *   fluctuation (ill-conditioned curvatures)
        *   saddle point

-   Improving SGD
    +   Momentum (introduce a velocity term and an additional hyperparameter mu)
    +   RMSProp (gradually reduce learning rate with control)
    +   Adam (reduce lr + momentum)

### CNN
#### Motivation
+ Efficiently extract local features using shared weights
+ robust against transformations like translation

#### Convolution layer structure
- Basics
    + kernel/filter
        * size
        * number of kernels = n_out (the number of output feature maps)
    + stride
        * the amount by which the filter shifts
    + padding
        * retain (increase) the size of the output volume

- Pooling
    + translation invariance
    + get abtract features

- Transposed Convolution
    - The opposite to convolution: expanding

* Atrous Convolution (Dilated convolution)
    - To get a larger receptive field without affecting the number of parameters to be trained

* Gated Convolution
    + convolution layer + gated linear unit
        * one conv layer without activation, another with logictic activation, gating unit calculates element-wise multiplication
    + alleviate gradient vanishing problem
        * The derivates of the layer without activation are constants

* Comparison to RNN
    + CNN easier to train and suffer less from gradient vanishing problem
    + Better suits problems where inference is not required

### RNN

#### Architectures
- LSTM
    
    |Component|input|activation function |meaning|
    |---|---|---|---|
    |input gate|current input and past hidden state|sigmoid|whehter the input is worth preserving|
    |forget gate|current input and past hidden state|sigmoid|whether the past memory cell is useful|
    |new memory cell|input word, past hidden state|tanh|A new memory|
    |final memory|forget gate and previous memory cell; input gate newly generated memory cell| |Gated final memory|
    |output gate|current input and past hidden state|sigmoid|how much final memory state to be passed onto the current hidden state|
    |current hidden state|output gate and final memory|tanh||

    * Memory intuition
        - long term memory: save past memory when current time state contains no useful information
        - short term memory: save current input information when previous memory is no longer useful
        - Long and short term memory: when forget gate and input gate both approach 1
    
    * Choices of activation functions
        - sigmoid: range(0, 1) for gates
        - tanh: range(-1, 1) center of distribution at 0, coverge faster because the gradient at around 0 is steeper

- GRU
    + similar to LSTM but have different gates
        * update gate: determine how much new memory (z) and the previous hidden state (1-z) to be feed into the new hidden state
        * reset gate: determine how much information from the previous hidden state are used in the generation of the new memory

#### Advantages of RNN
- detect long-term dependencies among the inputs
- deal with time series data
    + Memory allows for better understanding of the sequential data

#### Vanishing and exploding gradient problem
- Reason
    + If we unfold an RNN of T time steps, we get a T-layer weight-sharing feed-forward network. If we calculate the Jacodian matrices norms of the hidden states, there is an exponential term involved. It can grow extremely large and cause stack overflow, or vanish to 0 and eliminate the contribution of a faraway state. The former is gradient explosion, and the latter is gradient vanishing. 
- Solutions
    + Exploding gradient: 
        + set a threshold for the gradient and reset it when it exceeds the threshold
    + vanishing gradient: 
        +  use identity matrix initialization to avoid getting zeros when multiplying weight matrices together
        +  use ReLU instead of sigmoid activation
        +  LSTM and GRU suffer less from this problem

### Attention Mechanism
#### Motivation
- solve the problem of having to use fixed length vectors in encoder-decoder model
- align: identify which part is relevant
- translation: use relevant information to get appropriate output
- basic idea is to train context vectors by looping over encoder states and use softmax to generate a probablity distribution for each output word



